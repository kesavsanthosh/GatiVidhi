{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "id": "puEJ8HR0R6kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import docx\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "def extract_keywords_from_docx(docx_file):\n",
        "    doc = docx.Document(docx_file)\n",
        "    text = \" \".join(para.text for para in doc.paragraphs)\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    stopwords_set = set(stopwords.words(\"english\"))\n",
        "    keywords = [word for word in words if word not in stopwords_set and word.isalpha()]\n",
        "    return Counter(keywords)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_docx = \"/content/out.docx\"      # Replace with the path to your input DOCX file\n",
        "    keywords_count = 10                 # Number of most common keywords to display\n",
        "    keyword_counter = extract_keywords_from_docx(input_docx)\n",
        "    most_common_keywords = keyword_counter.most_common(keywords_count)\n",
        "    print(\"Most common keywords:\")\n",
        "    for keyword,count in most_common_keywords:\n",
        "        print(f\"{keyword}: {count} occurrences\")\n"
      ],
      "metadata": {
        "id": "BxKoCRDGR8eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at1yKk1yRUS7"
      },
      "outputs": [],
      "source": [
        "pip install python-docx PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import docx\n",
        "import fitz\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "def extract_keywords_from_docx(docx_file):\n",
        "    doc = docx.Document(docx_file)\n",
        "    text = \" \".join(para.text for para in doc.paragraphs)\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    stopwords_set = set(stopwords.words(\"english\"))\n",
        "    keywords = [word for word in words if word not in stopwords_set and word.isalpha()]\n",
        "    return Counter(keywords)\n",
        "\n",
        "def extract_paragraphs_containing_keywords_from_pdf(pdf_file, keywords):\n",
        "    pdf_document = fitz.open(pdf_file)\n",
        "    pdf_text = \"\"\n",
        "    selected_paragraphs = []\n",
        "\n",
        "    for page in pdf_document:\n",
        "        pdf_text += page.get_text()\n",
        "\n",
        "    paragraphs = re.split(r'\\n\\s*\\n', pdf_text)\n",
        "    for paragraph in paragraphs:\n",
        "        for keyword in keywords:\n",
        "            if keyword in paragraph.lower():\n",
        "                selected_paragraphs.append(paragraph)\n",
        "                break\n",
        "\n",
        "    pdf_document.close()\n",
        "    return selected_paragraphs\n",
        "\n",
        "def print_selected_paragraphs(paragraphs):\n",
        "    print(\"Selected paragraphs containing most common keywords:\")\n",
        "    for i, paragraph in enumerate(paragraphs, start=1):\n",
        "        print(f\"{i}. {paragraph}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_docx = \"/content/out.docx\"       # Replace with the path to your input DOCX file\n",
        "    input_pdf = \"/content/mv-84_merged.pdf\"         # Replace with the path to your input PDF file\n",
        "    keywords_count = 10                  # Number of most common keywords to display\n",
        "\n",
        "    keyword_counter = extract_keywords_from_docx(input_docx)\n",
        "    most_common_keywords = keyword_counter.most_common(keywords_count)\n",
        "    keywords = [keyword for keyword, _ in most_common_keywords]\n",
        "\n",
        "    selected_paragraphs = extract_paragraphs_containing_keywords_from_pdf(input_pdf, keywords)\n",
        "    print_selected_paragraphs(selected_paragraphs)\n"
      ],
      "metadata": {
        "id": "nvWbG_EfRXOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "YdKgl5QxRbCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')import re\n",
        "import docx\n",
        "import fitz\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.metrics import jaccard_distance\n",
        "import nltk\n",
        "\n",
        "\n",
        "def extract_keywords_from_docx(docx_file):\n",
        "    doc = docx.Document(docx_file)\n",
        "    text = \" \".join(para.text for para in doc.paragraphs)\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    stopwords_set = set(stopwords.words(\"english\"))\n",
        "    keywords = [word for word in words if word not in stopwords_set and word.isalpha()]\n",
        "    return Counter(keywords)\n",
        "\n",
        "def extract_important_sentences(paragraph):\n",
        "    # Tokenize the paragraph into sentences\n",
        "    sentences = sent_tokenize(paragraph)\n",
        "\n",
        "    # Sort sentences by length in descending order\n",
        "    sentences = sorted(sentences, key=len, reverse=True)\n",
        "\n",
        "    # Return the first two sentences (you can adjust this number)\n",
        "    return sentences[:2]\n",
        "\n",
        "def calculate_paragraph_similarity(paragraph1, paragraph2):\n",
        "    # Tokenize paragraphs into words (excluding stopwords)\n",
        "    words1 = [word.lower() for word in re.findall(r'\\b\\w+\\b', paragraph1) if word.lower() not in stopwords_set and word.isalpha()]\n",
        "    words2 = [word.lower() for word in re.findall(r'\\b\\w+\\b', paragraph2) if word.lower() not in stopwords_set and word.isalpha()]\n",
        "\n",
        "    # Calculate Jaccard similarity between the two paragraphs\n",
        "    similarity = 1 - jaccard_distance(set(words1), set(words2))\n",
        "    return similarity\n",
        "\n",
        "def extract_three_most_important_sentences(paragraphs):\n",
        "    # Calculate similarity between each pair of paragraphs\n",
        "    paragraph_scores = {}\n",
        "    for i in range(len(paragraphs)):\n",
        "        for j in range(i + 1, len(paragraphs)):\n",
        "            similarity = calculate_paragraph_similarity(paragraphs[i], paragraphs[j])\n",
        "            paragraph_scores[(i, j)] = similarity\n",
        "\n",
        "    # Find the three most similar paragraphs based on the highest similarity score\n",
        "    sorted_paragraphs = sorted(paragraphs, key=lambda p: sum(paragraph_scores[(i, j)] for i, j in paragraph_scores if i == paragraphs.index(p) or j == paragraphs.index(p)), reverse=True)\n",
        "    most_similar_paragraphs = sorted_paragraphs[:3]\n",
        "\n",
        "    # Extract the two most important sentences from the three most similar paragraphs\n",
        "    important_sentences_list = [extract_important_sentences(paragraph) for paragraph in most_similar_paragraphs]\n",
        "\n",
        "    return important_sentences_list\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # ... (previous code remains the same)\n",
        "\n",
        "    # Extract the two most important and similar sentences from three paragraphs\n",
        "    important_sentences_list = extract_three_most_important_sentences(selected_paragraphs)\n",
        "\n",
        "    print(\"Three most important and similar sentences:\")\n",
        "    for i, sentences in enumerate(important_sentences_list, start=1):\n",
        "        for j, sentence in enumerate(sentences, start=1):\n",
        "            print(f\"{i}. {sentence}\")\n"
      ],
      "metadata": {
        "id": "jXSFX04OSJUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def modify_list_for_plea(sentences_list, company_name):\n",
        "    # Example modifications, you can customize these based on your specific requirements\n",
        "    additional_statement = f\"The contents of the Para are denied in toto and {company_name} is put to strict proof of the same with cogent evidence.\"\n",
        "\n",
        "    plea = f\"{company_name}'s Written Statement\\n\\n\"\n",
        "    for i, sentences in enumerate(sentences_list, start=1):\n",
        "        cleaned_sentences = [sentence.replace(\"\\n\", \" \") for sentence in sentences]\n",
        "\n",
        "        # Add the additional statement to every sentence in the paragraph\n",
        "        modified_sentences = [f\"{sentence} {additional_statement}\" for sentence in cleaned_sentences]\n",
        "\n",
        "        # Combine sentences into a single paragraph\n",
        "        paragraph = \" \".join(modified_sentences)\n",
        "\n",
        "        # Add a phrase to indicate the company's stance\n",
        "        plea += f\"{i}. {paragraph} As representatives of {company_name}, we firmly stand by this statement.\"\n",
        "\n",
        "        # Use persuasive language to emphasize the significance of the statement\n",
        "        plea += f\"    It is imperative that this written statement is taken into serious consideration to provide a comprehensive understanding of the circumstances surrounding the case.\"\n",
        "\n",
        "        # Express cooperation and commitment to resolving the matter\n",
        "        plea += \"    We are fully committed to cooperating with all relevant parties and authorities to address and rectify the situation promptly.\"\n",
        "\n",
        "        # Add a new line for the next plea\n",
        "        plea += \"\\n\\n\"\n",
        "\n",
        "    # Add a closing statement expressing gratitude for the opportunity to submit the company's written statement\n",
        "    plea += f\"Thank you for considering {company_name}'s written statement, and we look forward to resolving this matter responsibly and expeditiously.\"\n",
        "\n",
        "    return plea\n",
        "\n",
        "# Assuming you already have 'important_sentences_list' as extracted from the previous step\n",
        "company_name = \"ABC Corporation\"  # Replace with the name of the company\n",
        "pleading_statement = modify_list_for_plea(important_sentences_list, company_name)\n",
        "\n",
        "# Print the modified plea for the company's written statement\n",
        "print(\"Modified plea for company's written statement:\")\n",
        "print(pleading_statement)\n"
      ],
      "metadata": {
        "id": "Rh7jN2vKSJeO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}